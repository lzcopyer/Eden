
# å…ˆå†³æ¡ä»¶

## åŸºçº¿é…ç½®

1. å…³é—­é˜²ç«å¢™ or è®¾ç½®ç™½åå•
2. é…ç½®æ—¶é—´åŒæ­¥
3. é…ç½®_SSH_å…å¯†ç™»å½•
4. é…ç½®ä¸»æœºåæ˜ å°„ or é…ç½®å†…ç½‘_DNS_

## æ‰€éœ€ä¾èµ–

Linuxå’ŒWindowsæ‰€éœ€è½¯ä»¶åŒ…æ‹¬:

	1. Java å¿…é¡»å®‰è£…ï¼Œå»ºè®®é€‰æ‹©Sunå…¬å¸å‘è¡Œçš„Javaç‰ˆæœ¬ã€‚
	2. sshÂ å¿…é¡»å®‰è£…å¹¶ä¸”ä¿è¯Â sshd ä¸€ç›´è¿è¡Œï¼Œä»¥ä¾¿ç”¨ Hadoop è„šæœ¬ç®¡ç†è¿œç«¯Hadoopå®ˆæŠ¤è¿›ç¨‹ã€‚

## ä¾èµ–å®‰è£…

``` Bash
apt-get install ssh rsync
```

# ä¼ªåˆ†å¸ƒå¼éƒ¨ç½²

## å‡†å¤‡å‡­è¯

**åˆ›å»º Principalï¼š** ä¸º HDFS å’Œ YARM æœåŠ¡åˆ›å»ºä¸»ä½“ã€‚

``` Bash
#!/bin/bash

HOSTNAMES='hadoop'
PRINCNAMES='nn dn snn HTTP rm nm hive'

for PRINCNAME in ${PRINCNAMES} do
    for HOSTNAME in ${HOSTNAMES} do
        kadmin.local -q "addprinc -randkey nn/localhost@EXAMPLE.COM"
    done
done
```

## å‡†å¤‡é…ç½®æ–‡ä»¶

### core-site.xml

``` XML
<?xml version="1.0"?>
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://_HOST:9000</value>
  </property>
  <property>
    <name>hadoop.security.authentication</name>
    <value>kerberos</value>
  </property>
  <property>
    <name>hadoop.security.authorization</name>
    <value>true</value>
  </property>
  <property>
    <name>hadoop.rpc.protection</name>
    <value>authentication</value>
  </property>
  <property>
    <name>hadoop.http.authentication.type</name>
    <value>kerberos</value>
  </property>
</configuration>
```

###  hdfs-site.xml

``` XML
<?xml version="1.0"?>
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <!-- é…ç½®é›†ç¾¤namenodeçš„kerberosè®¤è¯ -->
  <property>
    <name>dfs.block.access.token.enable</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.data.transfer.protection</name>
    <value>authentication</value>
  </property>
  <property>
    <name>dfs.namenode.kerberos.principal</name>
    <value>nn/_HOST@BIGDATA.COM</value>
  </property>
  <property>
    <name>dfs.namenode.keytab.file</name>
    <value>/etc/security/keytabs/hdfs.keytab</value>
  </property>
  <property>
    <name>dfs.web.authentication.kerberos.principal</name>
    <value>HTTP/_HOST@BIGDATA.COM</value>
  </property>
  <property>
    <name>dfs.web.authentication.kerberos.keytab</name>
    <value>/etc/security/keytabs/hdfs.keytab</value>
  </property>
  <!-- é…ç½®å¯¹NameNode Web UIçš„SSLè®¿é—® -->
  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.http.policy</name>
    <value>HTTPS_ONLY</value>
  </property>
  <property>
    <name>dfs.namenode.https-address</name>
    <value>0.0.0.0:50070</value>
  </property>
  <property>
    <name>dfs.permissions.supergroup</name>
    <value>hadoop</value>
    <description>The name of the group of super-users.</description>
  </property>
  <!-- é…ç½®é›†ç¾¤datanodeçš„kerberosè®¤è¯ -->
  <property>
    <name>dfs.datanode.kerberos.principal</name>
    <value>dn/_HOST@BIGDATA.COM</value>
  </property>
  <property>
    <name>dfs.datanode.keytab.file</name>
    <value>/etc/security/keytabs/hdfs.keytab</value>
  </property>
  <!-- é…ç½®datanode SASLé…ç½® -->
  <property>
    <name>dfs.datanode.data.dir.perm</name>
    <value>700</value>
  </property>
  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:50010</value>
  </property>
  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:50075</value>
  </property>
  <property>
    <name>dfs.data.transfer.protection</name>
    <value>integrity</value>
  </property>
  <!-- é…ç½®é›†ç¾¤secondarynamenodeçš„kerberosè®¤è¯ -->
  <property>
    <name>dfs.secondary.namenode.kerberos.principal</name>
    <value>snn/_HOST@BIGDATA.COM</value>
  </property>
  <property>
    <name>dfs.secondary.namenode.keytab.file</name>
    <value>/etc/security/keytabs/hdfs.keytab</value>
  </property>
  <!-- é…ç½®é›†ç¾¤journalnodeçš„kerberosè®¤è¯ -->
  <property>
    <name>dfs.journalnode.keytab.file</name>
    <value>/etc/security/keytab/hadoop.keytab</value>
  </property>
  <property>
    <name>dfs.journalnode.kerberos.principal</name>
    <value>hadoop/_HOST@HADOOP.COM</value>
  </property>
  <property>
    <name>dfs.journalnode.kerberos.internal.spnego.principal</name>
    <value>${dfs.web.authentication.kerberos.principal}</value>
  </property>
  <property>
    <name>dfs.journalnode.http-address</name>
    <value>0.0.0.0:8480</value>
  </property>
</configuration>
```

### yarn-site.xml

``` XML
<?xml version="1.0"?>
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>_HOST</value>
  </property>
  <property>
    <name>yarn.nodemanager.container-executor.class</name>
    <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
  </property>
  <!-- é…ç½®yarnçš„web ui è®¿é—®https -->
  <property>
    <name>yarn.http.policy</name>
    <value>HTTPS_ONLY</value>
  </property>
  <!-- æŒ‡å®šRM1çš„Webç«¯è®¿é—®åœ°å€ -->
  <property>
    <name>yarn.resourcemanager.webapp.address.rm1</name>
    <value>ha01:23188</value>
  </property>
  <!-- RM1 HTTPè®¿é—®åœ°å€,æŸ¥çœ‹é›†ç¾¤ä¿¡æ¯ -->
  <property>
    <name>yarn.resourcemanager.webapp.https.address.rm1</name>
    <value>ha01:23188</value>
  </property>
  <!-- æŒ‡å®šRM2çš„Webç«¯è®¿é—®åœ°å€ -->
  <property>
    <name>yarn.resourcemanager.webapp.address.rm2</name>
    <value>ha02:23188</value>
  </property>
  <!-- RM2 HTTPè®¿é—®åœ°å€,æŸ¥çœ‹é›†ç¾¤ä¿¡æ¯ -->
  <property>
    <name>yarn.resourcemanager.webapp.https.address.rm2</name>
    <value>ha02:23188</value>
  </property>
  <!-- å¼€å¯ YARN é›†ç¾¤çš„æ—¥å¿—èšåˆåŠŸèƒ½ -->
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
  <!-- YARN é›†ç¾¤çš„èšåˆæ—¥å¿—æœ€é•¿ä¿ç•™æ—¶é•¿ -->
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <!--7days:604800-->
    <value>86400</value>
  </property>
  <!-- é…ç½®yarnæäº¤çš„appç¨‹åºåœ¨hdfsä¸Šçš„æ—¥å¿—å­˜å‚¨è·¯å¾„ -->
  <property>
    <description>Where to aggregate logs to.</description>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/tmp/logs/yarn-nodemanager</value>
  </property>
  <!--YARN kerberos security-->
  <property>
    <name>yarn.resourcemanager.keytab</name>
    <value>/etc/security/keytab/hadoop.keytab</value>
  </property>
  <property>
    <name>yarn.resourcemanager.principal</name>
    <value>hadoop/_HOST@HADOOP.COM</value>
  </property>
  <property>
    <name>yarn.nodemanager.keytab</name>
    <value>/etc/security/keytab/hadoop.keytab</value>
  </property>
  <property>
    <name>yarn.nodemanager.principal</name>
    <value>hadoop/_HOST@HADOOP.COM</value>
  </property>
  <property>
    <name>yarn.nodemanager.container-executor.class</name>
    <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
  </property>
  <!--æ­¤å¤„çš„groupä¸ºnodemanagerç”¨æˆ·æ‰€å±ç»„-->
  <property>
    <name>yarn.nodemanager.linux-container-executor.group</name>
    <value>hadoop</value>
  </property>
</configuration>
```

### mapred-site.xml

``` XML
<?xml version="1.0"?>
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <!--mapred kerberos security-->
  <property>
    <name>mapreduce.jobhistory.keytab</name>
    <value>/etc/security/keytab/hadoop.keytab</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.principal</name>
    <value>hadoop/_HOST@HADOOP.COM</value>
  </property>
</configuration>
```

### container-executor.cfg

``` INI
#configured value of yarn.nodemanager.linux-container-executor.group
yarn.nodemanager.linux-container-executor.group=hadoop
#comma separated list of users who can not run applications
banned.users=root
#Prevent other super-users
min.user.id=500
#comma separated list of system users who CAN run applications
allowed.system.users=hadoop
```

> [!tip]
> é™¤äº†ä»¥ä¸Šçš„é…ç½®æ–‡ä»¶ï¼Œè¿˜éœ€è¦ä¿®æ”¹`hadoop-env.sh` `hive-env.sh` `yarn-env.sh`ç­‰ç¯å¢ƒé…ç½®æ–‡ä»¶ã€‚å¦‚æœ‰éœ€è¦è°ƒæ•´æ—¥å¿—è·¯å¾„å’Œçº§åˆ«å¯ä»¥ä¿®æ”¹å¯¹åº”ç»„ä»¶çš„`log4j.properties`ã€‚

## åˆå§‹åŒ–

``` Bash
$HADOOP_HOME/bin/hdfs namenode -format
```

## å¯åŠ¨æœåŠ¡

``` Bash
$HADOOP_HOME/sbin/start-all.sh
```

> [!tip]
> å¦‚æœå¯åŠ¨æœ‰å¼‚å¸¸å¯ä»¥å¼€å¯*DEBUG*æ¨¡å¼`export HADOOP_ROOT_LOGGER="DEBUG,console"`

# åˆ†å¸ƒå¼å®‰è£…

> åˆ†å¸ƒå¼HAé›†ç¾¤éœ€è¦Zookeeperé›†ç¾¤é…åˆï¼ŒZookeeperé›†ç¾¤çš„éƒ¨ç½²è¿™é‡Œå°±ä¸è®²è¿°ğŸ¤£

## Zookeeper é…ç½®Kerberosè®¤è¯

### zoo.cfg

åœ¨ $ZOOKEEPER_HOME/conf/zoo.cfg ä¸­è¿½åŠ ä»¥ä¸‹å†…å®¹

``` INI
kerberos.removeHostFromPrincipal=true
kerberos.removeRealmFromPrincipal=true
 
authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
jaasLoginRenew=3600000
```

### jaas.conf

æ–°å¢é…ç½®æ–‡ä»¶ $ZOOKEEPER_HOME/conf/jaas.conf

``` INI
Server {
 
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=true
  keyTab="/etc/security/keytab/hadoop.keytab" #keytabè¯ä¹¦çš„ä½ç½®
  storeKey=true
  useTicketCache=false
  principal="zookeeper/host_name@HADOOP.COM"; #è¿™é‡Œå¿…é¡»æ˜¯zookeeperï¼Œå¦åˆ™zkçš„å®¢æˆ·ç«¯åé¢å¯åŠ¨æŠ¥é”™
};
 
Client {
 
  com.sun.security.auth.module.Krb5LoginModule required
  useKeyTab=true
  keyTab="/etc/security/keytab/hadoop.keytab"
  storeKey=true
  useTicketCache=false
  principal="hadoop/host_name@HADOOP.COM";
};
```

### java.env

æ–°å¢é…ç½®æ–‡ä»¶ $ZOOKEEPER_HOME/conf/java.env

``` INI
export JVMFLAGS="-Djava.security.auth.login.config=$ZOOKEEPER_HOME/conf/jaas.conf"
```

> [!tip] 
> ä¿®æ”¹é…ç½®ä¹‹åé‡å¯ Zookeeper é›†ç¾¤å³å¯

# å‚è€ƒé“¾æ¥

[apache-hadoop](https://hadoop.apache.org/)