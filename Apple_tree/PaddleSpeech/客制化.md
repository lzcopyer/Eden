# 文本前端优化

> 文本前端代码路径：paddlespeech/t2s/frontend/zh_normalization

- **数字语音合成优化**
	  1. 优化`num.py`，优化之后会正常返回带有多个小数点的数字串，而不会在第二个小数点之后的数字前面添加零
 
	``` Python
	# 文件 num.py
	
	# ... (在 RE_YEAR 附近添加) ...
	# 年份表达式：四位数字后跟 '年'
	RE_YEAR = re.compile(r'(\d{4})年')
	
	
	# IP 地址表达式：匹配 4 组 1-3 位数字，用点号 '.' 分隔
	# 注意：这里只匹配格式，不验证数字范围（0-255）
	RE_IP_ADDRESS = re.compile(r'(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})')
	
	
	def replace_year(match) -> str:
	# ... (replace_year 函数保持不变) ...
	
	# 添加处理 IP 地址的函数
	def replace_ip_address(match) -> str:
	    """
	    Args:
	        match (re.Match)
	    Returns:
	        str
	    """
	    # 匹配组依次是 IP 地址的四个部分
	    segments = [match.group(i) for i in range(1, 5)]
    
	    verbalized_segments = []
	    for seg in segments:
	        # IP 地址采用逐位读法，例如 '10' -> '一零'，'210' -> '二一零'
	        # 不使用 alt_one=True (不把 '一' 转换为 '幺')
	        verbalized_segments.append(verbalize_digit(seg, alt_one=False))
	        
	    # 用 '点' 连接四个部分
	    result = "点".join(verbalized_segments)
	    return result
	
	def _get_value(value_string: str, use_zero: bool=True) -> List[str]:
	# ... (其他代码保持不变) ...
	```
	
	  2. 在 `text_normlization.py` 中，导入新的正则表达式和替换函数，并将其插入到 `normalize_sentence` 方法的替换流程中。
	
	``` Python
	# 文件 text_normlization.py
	
	# ... (在 RE_RANGE 附近导入) ...
	from .num import RE_RANGE
	from .num import RE_YEAR
	from .num import RE_IP_ADDRESS # <--- 新增导入
	from .num import replace_default_num
	from .num import replace_frac
	# ... (在 replace_range 附近导入) ...
	from .num import replace_range
	from .num import replace_year
	from .num import replace_ip_address # <--- 新增导入
	from .phonecode import RE_MOBILE_PHONE
	# ...
	
	# 文件 text_normlization.py
	
	    def normalize_sentence(self, sentence: str) -> str:
	        # basic character conversions
	        # ... (保持不变) ...
	
	        # number related NSW verbalization
	        sentence = RE_DATE.sub(replace_date, sentence)
	        sentence = RE_DATE2.sub(replace_date2, sentence)
	
	        # range first
	        sentence = RE_TIME_RANGE.sub(replace_time, sentence)
	        sentence = RE_TIME.sub(replace_time, sentence)
	
	        sentence = RE_YEAR.sub(replace_year, sentence)
	        
	        # <--- 在这里添加 IP 地址替换
	        sentence = RE_IP_ADDRESS.sub(replace_ip_address, sentence) 
	
	        sentence = RE_TEMPERATURE.sub(replace_temperature, sentence)
	        sentence = replace_measure(sentence)
	        sentence = RE_FRAC.sub(replace_frac, sentence)
	        # ... (其他替换规则保持不变) ...
	```

- **自定义字典**
	优化`mix_frontend.py`引入自定义词典
	``` Python
	# 文件 mix_frontend.py
	
	# ... (其他导入保持不变) ...
	
	class MixFrontend():
	    def __init__(self,
	                 g2p_model="pypinyin",
	                 phone_vocab_path=None,
	                 tone_vocab_path=None,
	                 custom_dict: Dict[str, str] = None): # <-- 新增参数
	        
	        # ... (原有初始化代码保持不变) ...
	
	        # 添加自定义词典，用于处理专有名词
	        # ⚠️ 这里的 dict_path/loading 逻辑需要您根据实际文件情况实现
	        # 为简化，我在此直接硬编码一个示例词典，但最佳实践是像在 text_normlization.py 中那样加载文件
	        self.custom_tech_dict = {
	            "mysql": "my sequel",  # 读作 My Sequel
	            "MySQL": "my sequel",
	            "nginx": "engine x",   # 读作 Engine X
	            "apache": "apache",    # 读作 Apache (保留，确保被识别)
	            "ASR": "A S R",        # 缩写，读作字母
	            "G P U": "G P U"       # 确保 G P U 作为完整英文片段处理
	        }
	        if custom_dict:
	            self.custom_tech_dict.update(custom_dict)
	        
	    # ... (其他方法保持不变) ...
	    # 文件 mix_frontend.py
		
		# ... (在 is_other 后面添加新方法) ...
		
	    def _apply_custom_dict_replacement(self, text: str) -> str:
	        """
		        Apply custom dictionary replacement for technical terms before language split.
	        """
	        # 按键长度降序排序，确保长词优先匹配 (例如 "ASR" 优先于 "A")
	        sorted_keys = sorted(self.custom_tech_dict.keys(), key=len, reverse=True)
	        
	        for key in sorted_keys:
	            value = self.custom_tech_dict[key]
	            # 使用 re.sub 并确保匹配完整的词汇（在中文语境下，词汇通常被空格或标点符号分隔）
	            # 由于这里假设只替换英文专有名词，可以使用 \b (word boundary)
	            # 但为安全起见，且避免与中文混淆，我们使用简单的字符串替换，这要求字典中的键和值与文本精确匹配
	            text = text.replace(key, value)
	            
	        return text
		
	    def split_by_lang(self, text: str) -> List[str]:
		# ... (split_by_lang 保持不变) ...
	
	
		# ... (在 get_input_ids 方法内部修改) ...
		
	    def get_input_ids(self,
	                      sentence: str,
	                      merge_sentences: bool=False,
	                      get_tone_ids: bool=False,
	                      add_sp: bool=True,
	                      to_tensor: bool=True) -> Dict[str, List[paddle.Tensor]]:
        
	        # XML Document Object Model (DOM)
	        doms = MixTextProcessor.get_dom_split(sentence)
		
	        lang_splits = []
	        for dom in doms:
	            if dom.lower().startswith("<say-as pinyin="):
	                # `<say-as pinyin=` for zh lang
	                lang_splits.append((dom, "zh"))
	            else:
	                # <-- 新增：应用词典替换
	                dom_replaced = self._apply_custom_dict_replacement(dom)
	                
	                # process zh, en and zh/en
	                lang_splits.extend(self.split_by_lang(dom_replaced)) # <-- 使用替换后的文本
		
	        # ... (后续代码保持不变) ...
	```

# 数据集训练

## 准备词汇表

需要准备`csv`或者`json`格式的词汇表，以`it.csv`为例

``` csv
text,label
"我喜欢这个电影。",1
"这个产品质量很差。",0
"今天天气真好！",1
```

## 扩展词汇表

> 这种方法是为模型添加全新的词汇。这通常需要从零开始预训练（pre-training）一个BERT模型，或者继续预训练（continue pre-training）一个现有模型。这个过程计算量很大，但效果最好，尤其是在拥有领域数据量非常大的情况下。

``` Python
from transformers import BertTokenizer, BertForMaskedLM, LineByLineTextDataset, Trainer, TrainingArguments

# 1. 加载基础分词器和模型
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
model = BertForMaskedLM.from_pretrained("bert-base-chinese")

# 2. 定义您想添加的新词
new_tokens = ["MySQL", "Nginx", "I/O", "NFS", "Kubernetes", "Docker", "Git", "API"]
num_added_toks = tokenizer.add_tokens(new_tokens)
model.resize_token_embeddings(len(tokenizer)) # 调整模型嵌入层大小

# 3. 准备数据和训练
# 假设您的IT语料文件名为 'it_corpus.txt'
dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path="it_corpus.txt",
    block_size=128
)

training_args = TrainingArguments(
    output_dir="./my_it_bert",
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=16,
    save_steps=10_000,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

trainer.train()
```

## 微调模型

> 这种方法更加实用，并且计算成本低得多。不需要修改模型的词汇表，而是通过在包含这些术语的特定任务数据集上**微调**模型，让模型学会如何处理它们。

``` Python
from datasets import load_dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

# 1. 加载基础分词器和模型
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
model = BertForSequenceClassification.from_pretrained("bert-base-chinese", num_labels=2) # 假设二分类

# 2. 准备自定义数据集（这里假设您已有一个包含IT术语的CSV文件 'it_data.csv'）
# 格式: text, label
# "MySQL性能很棒，值得一用。",1
# "NFS的配置有些复杂。",0
dataset = load_dataset('csv', data_files='it_data.csv')

def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
tokenized_dataset = tokenized_dataset.remove_columns(["text"])

# 3. 微调模型
training_args = TrainingArguments(
    output_dir="./it_finetuned_model",
    num_train_epochs=3,
    per_device_train_batch_size=8,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
)

trainer.train()
```